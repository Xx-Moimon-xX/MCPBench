import json
import dspy
from typing import List, Tuple, Optional
from langProBe.program_utils import call_lm, ProcessManager, MCPCall
import langProBe.constants as constants
import logging
import re
import string
import warnings
import os
import logging
import numpy as np


EVALUATE_PROMPT = """For the following question: {question}

Please determine whether the predicted answer is correct. Consider it correct if it addresses the key information:

Predicted answer: {prediction}
Correct answer: {ground_truth}

Return only True or False."""


## THIS IS WHAT I NEED TO CHANGE TO GET THE EVALUATION TO WORK!!!
def evaluate_final_answer(
            question: str, 
            ground_truth: str, 
            tools_required: List[str],
            tools_called: List[MCPCall],
            prediction: str, 
            manager: ProcessManager,
            logger: logging.Logger,
            ) -> Tuple[bool, Optional[str]]:
    prompt = EVALUATE_PROMPT.format(question=question, prediction=prediction, ground_truth=ground_truth)
    messages = [
        {
            constants.ROLE: constants.USER,
            constants.CONTENT: prompt
        }
    ]
    logger.info(f"Starting evaluation of final answer")
    logger.info(f"question: {question}")
    logger.info(f"expected_response: {ground_truth[:50]}")
    logger.info(f"Prediction: {prediction[:50]}")
    response_content, _, _ = call_lm(messages, manager, logger, temperature=0.01)

    tool_calling_success = True
    ## Checking if the required tools were called.
    if tools_called:
        called_tool_names = [call.mcp_tool_name for call in tools_called]
        for tool in tools_required:
            if tool not in called_tool_names:
                print(f"Tool {tool} was not called.")
                tool_calling_success = False
                # return False, None, False
    else:
        tool_calling_success = False
    
    ## If I want to add other tool calling stuff like how many tools called and which arguments were passed, I can do this here.

    if response_content is None:
        return False, None, tool_calling_success
    
    success = "true" in response_content.lower() and tool_calling_success

    return success, response_content, tool_calling_success


def normalize_number_str(number_str: str) -> float:
    # we replace these common units and commas to allow
    # conversion to float
    for char in ["$", "%", ","]:
        number_str = number_str.replace(char, "")
    try:
        return float(number_str)
    except ValueError:
        print(f"String {number_str} cannot be normalized to number str.")
        return float("inf")


def split_string(
        s: str,
        char_list: list[str] = [",", ";"],
) -> list[str]:
    pattern = f"[{''.join(char_list)}]"
    return re.split(pattern, s)

def normalize_str(input_str, remove_punct=True) -> str:
    """
    Normalize a string by:
    - Removing all white spaces
    - Optionally removing punctuation (if remove_punct is True)
    - Converting to lowercase
    Parameters:
    - input_str: str, the string to normalize
    - remove_punct: bool, whether to remove punctuation (default: True)
    Returns:
    - str, the normalized string
    """
    # Remove all white spaces. Required e.g for seagull vs. sea gull
    no_spaces = re.sub(r"\s", "", input_str)

    # Remove punctuation, if specified.
    if remove_punct:
        translator = str.maketrans("", "", string.punctuation)
        return no_spaces.lower().translate(translator)
    else:
        return no_spaces.lower()


def question_scorer(
        model_answer: str,
        ground_truth: str,
        logger: logging.Logger
) -> Tuple[bool, Optional[str]]:
    """
    Evaluates the model's answer against the ground truth, handling different data types.

    It automatically detects if the ground truth is a number, a list (comma or
    semicolon-separated), or a plain string, and applies the appropriate
    normalization and comparison logic.

    - For numbers, it compares them as floats.
    - For lists, it compares for equal length and then element by element.
    - For strings, it performs a normalized string comparison.

    Args:
        model_answer: The answer generated by the model.
        ground_truth: The correct answer.
        logger: The logger instance for logging evaluation details.

    Returns:
        A boolean indicating if the answer is correct.
    """

    def is_float(element: any) -> bool:
        try:
            float(element)
            return True
        except ValueError:
            return False

    if model_answer is None:
        model_answer = "None"
        logger.debug("Model answer is None. Converted to string 'None'.")

    # If ground truth is a number
    if is_float(ground_truth):
        info = f"Evaluating '{model_answer}' as a number."
        logger.info(info)
        normalized_answer = normalize_number_str(model_answer)
        try:
            result = normalized_answer == float(ground_truth)
            logger.debug(f"Normalized model answer: {normalized_answer}, Ground truth: {ground_truth}, Result: {result}")
            return result
        except ValueError as e:
            error_msg = f"Normalization error: {e}"
            logger.error(error_msg)
            return False

    # If ground truth is a list
    elif any(char in ground_truth for char in [",", ";"]):
        info = f"Evaluating '{model_answer}' as a comma/semi-colon separated list."
        logger.info(info)

        gt_elems = split_string(ground_truth)
        ma_elems = split_string(model_answer)
        logger.debug(f"Ground truth elements: {gt_elems}")
        logger.debug(f"Model answer elements: {ma_elems}")

        # Check if lengths are the same
        if len(gt_elems) != len(ma_elems):
            warning_msg = "Answer lists have different lengths."
            logger.warning(warning_msg)
            return False

        # Compare each element as float or string
        comparisons = []
        for idx, (ma_elem, gt_elem) in enumerate(zip(ma_elems, gt_elems), start=1):
            if is_float(gt_elem):
                try:
                    normalized_ma_elem = normalize_number_str(ma_elem)
                    comparison = normalized_ma_elem == float(gt_elem)
                    logger.debug(f"Element {idx}: Normalized model answer element '{normalized_ma_elem}' == Ground truth element '{float(gt_elem)}': {comparison}")
                except ValueError as e:
                    error_msg = f"Normalization error at element {idx}: {e}"
                    logger.error(error_msg)
                    return False
            else:
                normalized_ma = normalize_str(ma_elem, remove_punct=False)
                normalized_gt = normalize_str(gt_elem, remove_punct=False)
                comparison = normalized_ma == normalized_gt
                logger.debug(f"Element {idx}: Normalized model answer element '{normalized_ma}' == Ground truth element '{normalized_gt}': {comparison}")
            comparisons.append(comparison)

        all_correct = all(comparisons)
        if not all_correct:
            detail_msg = "Mismatch found in list elements."
            logger.info(detail_msg)
            return all_correct
        logger.debug("All list elements match the ground truth.")
        return all_correct

    # If ground truth is a string
    else:
        info = f"Evaluating '{model_answer}' as a string."
        logger.info(info)
        normalized_ma = normalize_str(model_answer)
        normalized_gt = normalize_str(ground_truth)
        result = normalized_ma == normalized_gt
        logger.debug(f"Normalized model answer: '{normalized_ma}' == Normalized ground truth: '{normalized_gt}': {result}")
        return result

def mcp_metric(example: dspy.Example, pred: dspy.Prediction):
    '''
    Given a prediction, returns the success of the prediction. In this format because the DSPy Evaluate class expects it.
    Returns True if the prediction is successful, False otherwise.
    '''
    return pred.success


# EVAL_PROMPT_1 = """You are evaluating an LLM response against a prompt and expected answer. You have two evaluation tasks:

# **Task 1: Prompt Adherence**
# Evaluate if the response appropriately addresses the given prompt, including cases where the expected response indicates a failure.
# - Score 5: Fully addresses all aspects of the prompt, including correct identification of failures if applicable
# - Score 4: Addresses most aspects with minor gaps (may miss minor failure details)
# - Score 3: Addresses some aspects but misses key elements or misrepresents failure cases
# - Score 2: Minimally addresses the prompt or incorrectly describes failures
# - Score 1: Fails to address the prompt meaningfully

# **Task 2: Content Accuracy** 
# Evaluate if the response conveys the same semantic meaning and core content as the expected response, including failure scenarios.
# - Score 5: Conveys the same semantic meaning and captures all core concepts from the expected response, even if phrased differently
# - Score 4: Conveys similar semantic meaning with most core concepts, minor differences in emphasis or detail
# - Score 3: Conveys the general meaning but misses some important concepts or has notable semantic gaps
# - Score 2: Partially aligns with expected meaning but has significant conceptual differences or omissions
# - Score 1: Conveys different semantic meaning or contradicts the core concepts of the expected response

# [BEGIN DATA]
# [Prompt]: {prompt}
# [Response]: {response}
# [Expected Response]: {expected_response}
# [END DATA]

# For each task:
# 1. Analyze the relevant comparison
# 2. Assign a score (1-5) using the rubric above
# 3. Provide a yes/no answer (Prompt Adherence: "Does it address the prompt?" | Content Accuracy: "Does it convey the same semantic meaning?")
# 4. Give brief reasoning

# Final score = Prompt Adherence + Content Accuracy (max 10)

# Return as a JSON object with the following structure:
# {{
#   "prompt_adherence_score": <1-5>,
#   "prompt_adherence_answer": "<yes/no>",
#   "prompt_adherence_reasoning": "<brief explanation>",
#   "content_accuracy_score": <1-5>,
#   "content_accuracy_answer": "<yes/no>",
#   "content_accuracy_reasoning": "<brief explanation>",
#   "final_score": <2-10>
# }}
# """

# def eval_prompt_1_metric(example: dspy.Example, pred: dspy.Prediction):
#     """
#     Evaluates a prediction using the eval_prompt_1 evaluation prompt.
#     Returns True if final_score >= 6, False otherwise.

#     I DONT REALLY KNOW HOW TO USE THIS FUNCTION.
#     """
#     if not hasattr(pred, 'answer') or not pred.answer:
#         return False
    
    # prompt_text = EVAL_PROMPT_1.format(
    #     prompt=example.question,
    #     response=pred.answer,
    #     expected_response=example.answer
    # )
    
    # Create a simple evaluation using the existing infrastructure
    # This is a simplified version - in practice you'd want to use the full ProcessManager
    # try:
    #     # For now, we'll use a simple heuristic based on string similarity
    #     # In a real implementation, you'd call an LLM with the prompt_text
        
    #     # Simple fallback scoring based on string similarity
    #     response_lower = pred.answer.lower()
    #     expected_lower = example.answer.lower()
        
    #     # Basic similarity check
    #     if response_lower == expected_lower:
    #         return True
    #     elif any(word in response_lower for word in expected_lower.split()):
    #         return True
    #     else:
    #         return False
            
    # except Exception as e:
    #     return False



def extract_questions(data, key):
    """Extract specified field (such as Prompt or question) from data for comparison"""
    questions = set()
    for item in data:
        questions.add(item[key])
    return questions

def find_missing_entries(data_a, data_b):
    # data_a is the original data, data_b is the data that has been processed
 
    questions_in_b = extract_questions(data_b, 'question')

    # Find entries in A that do not exist in B
    missing_entries = [item for item in data_a if item['Prompt'] not in questions_in_b]

    return missing_entries

import logging

import os
import logging

def replace_logger_filehandler(new_log_name):
    """
    Replace existing FileHandler in logger and preserve the original formatter for each logger.
    Also delete the original log files.

    :param new_log_name: New log file name (without suffix)
    """

    def update_handler(logger, file_suffix):
        old_log_paths = []
        formatter = None
        for handler in logger.handlers:
            if isinstance(handler, logging.FileHandler):
                if formatter is None:
                    formatter = handler.formatter
                old_log_paths.append(handler.baseFilename)

        for handler in list(logger.handlers):
            if isinstance(handler, logging.FileHandler):
                handler.close()
                logger.removeHandler(handler)

        for log_path in old_log_paths:
            if os.path.exists(log_path):
                try:
                    os.remove(log_path)
                except Exception as e:
                    pass

        if logger.name == 'MCPPredictRunLogger':
            new_name = new_log_name.replace("message", "run")
        else:
            new_name = new_log_name 

        new_handler = logging.FileHandler(f"{new_name}.{file_suffix}", mode='a', encoding='utf-8')
        if formatter:
            new_handler.setFormatter(formatter)
        logger.addHandler(new_handler)

    run_logger = logging.getLogger('MCPPredictRunLogger')
    update_handler(run_logger, 'log')

    message_logger = logging.getLogger('MCPPredictMessageLogger')
    update_handler(message_logger, 'jsonl')


# def evaluate_final_answer_eval1(
#             question: str, 
#             ground_truth: str, 
#             prediction: str, 
#             manager: ProcessManager,
#             logger: logging.Logger,
#             ) -> Tuple[bool, Optional[str]]:
#     prompt = EVAL_PROMPT_1.format(prompt=question, response=prediction, expected_response=ground_truth)
#     messages = [
#         {
#             constants.ROLE: constants.USER,
#             constants.CONTENT: prompt
#         }
#     ]
#     logger.info(f"Starting evaluation of final answer with rubric")
#     logger.info(f"question: {question}")
#     logger.info(f"expected_response: {ground_truth[:50]}")
#     logger.info(f"Prediction: {prediction[:50]}")
#     response_content, _, _ = call_lm(messages, manager, logger, temperature=0.01)
    
#     json_str = ""
#     try:
#         # Try to extract JSON from markdown code block if present
#         json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_content, re.DOTALL)
#         if json_match:
#             json_str = json_match.group(1)
#         else:
#             # Fallback for raw JSON possibly with leading/trailing text
#             json_start = response_content.find('{')
#             json_end = response_content.rfind('}')
#             if json_start != -1 and json_end != -1:
#                 json_str = response_content[json_start:json_end+1]
#             else:
#                 raise json.JSONDecodeError("No JSON object found in response", response_content, 0)

#         scores_data = json.loads(json_str)
#         # print(f"DEBUG: scores_data: {scores_data}")
        
#         prompt_adherence_score = scores_data.get("prompt_adherence_score")
#         content_accuracy_score = scores_data.get("content_accuracy_score")
#         final_score = scores_data.get("final_score")

#         logger.info(f"Extracted scores: Prompt Adherence={prompt_adherence_score}, Content Accuracy={content_accuracy_score}, Final={final_score}")

#         if final_score is None:
#             logger.error("Could not find 'final_score' in the LLM response.")
#             return False, "Missing 'final_score' in response"

#         # Success is defined as final_score >= 6 (based on eval_prompt_1_metric docstring)
#         is_success = int(final_score) >= 6
        
#         return is_success, json.dumps(scores_data)

#     except json.JSONDecodeError:
#         error_msg = f"Failed to decode JSON from LLM response: {response_content}"
#         logger.error(error_msg)
#         return False, error_msg
#     except (KeyError, TypeError) as e:
#         error_msg = f"Error accessing scores from parsed JSON: {e}. Data: {json_str}"
#         logger.error(error_msg)
#         return False, error_msg



if __name__ == "__main__":
    print(question_scorer("123", "123"))
    