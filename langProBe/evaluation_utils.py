import json
import dspy
from typing import List, Tuple, Optional
from langProBe.program_utils import call_lm, ProcessManager
import langProBe.constants as constants
import logging
import re
import string
import warnings
import os
import logging
import numpy as np


EVALUATE_PROMPT = """For the following question: {question}

Please determine whether the predicted answer is correct. Consider it correct if it addresses the key information:

Predicted answer: {prediction}
Correct answer: {ground_truth}

Return only True or False."""


## THIS IS WHAT I NEED TO CHANGE TO GET THE EVALUATION TO WORK!!!
def evaluate_final_answer(
            question: str, 
            ground_truth: str, 
            prediction: str, 
            manager: ProcessManager,
            logger: logging.Logger,
            ) -> Tuple[bool, Optional[str]]:
    prompt = EVALUATE_PROMPT.format(question=question, prediction=prediction, ground_truth=ground_truth)
    messages = [
        {
            constants.ROLE: constants.USER,
            constants.CONTENT: prompt
        }
    ]
    logger.info(f"Starting evaluation of final answer")
    logger.info(f"question: {question}")
    logger.info(f"ground_truth: {ground_truth}")
    logger.info(f"prediction: {prediction}")
    response_content, _, _ = call_lm(messages, manager, logger, temperature=0.01)
    return "true" in response_content.lower()


def normalize_number_str(number_str: str) -> float:
    # we replace these common units and commas to allow
    # conversion to float
    for char in ["$", "%", ","]:
        number_str = number_str.replace(char, "")
    try:
        return float(number_str)
    except ValueError:
        print(f"String {number_str} cannot be normalized to number str.")
        return float("inf")


def split_string(
        s: str,
        char_list: list[str] = [",", ";"],
) -> list[str]:
    pattern = f"[{''.join(char_list)}]"
    return re.split(pattern, s)

def normalize_str(input_str, remove_punct=True) -> str:
    """
    Normalize a string by:
    - Removing all white spaces
    - Optionally removing punctuation (if remove_punct is True)
    - Converting to lowercase
    Parameters:
    - input_str: str, the string to normalize
    - remove_punct: bool, whether to remove punctuation (default: True)
    Returns:
    - str, the normalized string
    """
    # Remove all white spaces. Required e.g for seagull vs. sea gull
    no_spaces = re.sub(r"\s", "", input_str)

    # Remove punctuation, if specified.
    if remove_punct:
        translator = str.maketrans("", "", string.punctuation)
        return no_spaces.lower().translate(translator)
    else:
        return no_spaces.lower()


def question_scorer(
        model_answer: str,
        ground_truth: str,
        logger: logging.Logger
) -> Tuple[bool, Optional[str]]:
    """
    Evaluates the model's answer against the ground truth, handling different data types.

    It automatically detects if the ground truth is a number, a list (comma or
    semicolon-separated), or a plain string, and applies the appropriate
    normalization and comparison logic.

    - For numbers, it compares them as floats.
    - For lists, it compares for equal length and then element by element.
    - For strings, it performs a normalized string comparison.

    Args:
        model_answer: The answer generated by the model.
        ground_truth: The correct answer.
        logger: The logger instance for logging evaluation details.

    Returns:
        A boolean indicating if the answer is correct.
    """

    def is_float(element: any) -> bool:
        try:
            float(element)
            return True
        except ValueError:
            return False

    if model_answer is None:
        model_answer = "None"
        logger.debug("Model answer is None. Converted to string 'None'.")

    # If ground truth is a number
    if is_float(ground_truth):
        info = f"Evaluating '{model_answer}' as a number."
        logger.info(info)
        normalized_answer = normalize_number_str(model_answer)
        try:
            result = normalized_answer == float(ground_truth)
            logger.debug(f"Normalized model answer: {normalized_answer}, Ground truth: {ground_truth}, Result: {result}")
            return result
        except ValueError as e:
            error_msg = f"Normalization error: {e}"
            logger.error(error_msg)
            return False

    # If ground truth is a list
    elif any(char in ground_truth for char in [",", ";"]):
        info = f"Evaluating '{model_answer}' as a comma/semi-colon separated list."
        logger.info(info)

        gt_elems = split_string(ground_truth)
        ma_elems = split_string(model_answer)
        logger.debug(f"Ground truth elements: {gt_elems}")
        logger.debug(f"Model answer elements: {ma_elems}")

        # Check if lengths are the same
        if len(gt_elems) != len(ma_elems):
            warning_msg = "Answer lists have different lengths."
            logger.warning(warning_msg)
            return False

        # Compare each element as float or string
        comparisons = []
        for idx, (ma_elem, gt_elem) in enumerate(zip(ma_elems, gt_elems), start=1):
            if is_float(gt_elem):
                try:
                    normalized_ma_elem = normalize_number_str(ma_elem)
                    comparison = normalized_ma_elem == float(gt_elem)
                    logger.debug(f"Element {idx}: Normalized model answer element '{normalized_ma_elem}' == Ground truth element '{float(gt_elem)}': {comparison}")
                except ValueError as e:
                    error_msg = f"Normalization error at element {idx}: {e}"
                    logger.error(error_msg)
                    return False
            else:
                normalized_ma = normalize_str(ma_elem, remove_punct=False)
                normalized_gt = normalize_str(gt_elem, remove_punct=False)
                comparison = normalized_ma == normalized_gt
                logger.debug(f"Element {idx}: Normalized model answer element '{normalized_ma}' == Ground truth element '{normalized_gt}': {comparison}")
            comparisons.append(comparison)

        all_correct = all(comparisons)
        if not all_correct:
            detail_msg = "Mismatch found in list elements."
            logger.info(detail_msg)
            return all_correct
        logger.debug("All list elements match the ground truth.")
        return all_correct

    # If ground truth is a string
    else:
        info = f"Evaluating '{model_answer}' as a string."
        logger.info(info)
        normalized_ma = normalize_str(model_answer)
        normalized_gt = normalize_str(ground_truth)
        result = normalized_ma == normalized_gt
        logger.debug(f"Normalized model answer: '{normalized_ma}' == Normalized ground truth: '{normalized_gt}': {result}")
        return result

def mcp_metric(example: dspy.Example, pred: dspy.Prediction):
    '''
    Evaluates a prediction using the MCP metric.
    Returns True if the prediction is successful, False otherwise.
    '''
    return pred.success


EVAL_PROMPT_1 = """You are evaluating an LLM response against a prompt and expected answer. You have two evaluation tasks:

**Task 1: Prompt Adherence**
Evaluate if the response appropriately addresses the given prompt, including cases where the expected response indicates a failure.
- Score 5: Fully addresses all aspects of the prompt, including correct identification of failures if applicable
- Score 4: Addresses most aspects with minor gaps (may miss minor failure details)
- Score 3: Addresses some aspects but misses key elements or misrepresents failure cases
- Score 2: Minimally addresses the prompt or incorrectly describes failures
- Score 1: Fails to address the prompt meaningfully

**Task 2: Content Accuracy** 
Evaluate if the response conveys the same semantic meaning and core content as the expected response, including failure scenarios.
- Score 5: Conveys the same semantic meaning and captures all core concepts from the expected response, even if phrased differently
- Score 4: Conveys similar semantic meaning with most core concepts, minor differences in emphasis or detail
- Score 3: Conveys the general meaning but misses some important concepts or has notable semantic gaps
- Score 2: Partially aligns with expected meaning but has significant conceptual differences or omissions
- Score 1: Conveys different semantic meaning or contradicts the core concepts of the expected response

[BEGIN DATA]
[Prompt]: {prompt}
[Response]: {response}
[Expected Response]: {expected_response}
[END DATA]

For each task:
1. Analyze the relevant comparison
2. Assign a score (1-5) using the rubric above
3. Provide a yes/no answer (Prompt Adherence: "Does it address the prompt?" | Content Accuracy: "Does it convey the same semantic meaning?")
4. Give brief reasoning

Final score = Prompt Adherence + Content Accuracy (max 10)

Return as a JSON object with the following structure:
{{
  "prompt_adherence_score": <1-5>,
  "prompt_adherence_answer": "<yes/no>",
  "prompt_adherence_reasoning": "<brief explanation>",
  "content_accuracy_score": <1-5>,
  "content_accuracy_answer": "<yes/no>",
  "content_accuracy_reasoning": "<brief explanation>",
  "final_score": <2-10>
}}
"""

def eval_prompt_1_metric(example: dspy.Example, pred: dspy.Prediction):
    """
    Evaluates a prediction using the eval_prompt_1 evaluation prompt.
    Returns True if final_score >= 6, False otherwise.

    I DONT REALLY KNOW HOW TO USE THIS FUNCTION.
    """
    if not hasattr(pred, 'answer') or not pred.answer:
        return False
    
    prompt_text = EVAL_PROMPT_1.format(
        prompt=example.question,
        response=pred.answer,
        expected_response=example.answer
    )
    
    # Create a simple evaluation using the existing infrastructure
    # This is a simplified version - in practice you'd want to use the full ProcessManager
    try:
        # For now, we'll use a simple heuristic based on string similarity
        # In a real implementation, you'd call an LLM with the prompt_text
        
        # Simple fallback scoring based on string similarity
        response_lower = pred.answer.lower()
        expected_lower = example.answer.lower()
        
        # Basic similarity check
        if response_lower == expected_lower:
            return True
        elif any(word in response_lower for word in expected_lower.split()):
            return True
        else:
            return False
            
    except Exception as e:
        return False



def extract_questions(data, key):
    """Extract specified field (such as Prompt or question) from data for comparison"""
    questions = set()
    for item in data:
        questions.add(item[key])
    return questions

def find_missing_entries(data_a, data_b):
    # data_a is the original data, data_b is the data that has been processed
 
    questions_in_b = extract_questions(data_b, 'question')

    # Find entries in A that do not exist in B
    missing_entries = [item for item in data_a if item['Prompt'] not in questions_in_b]

    return missing_entries

import logging

import os
import logging

def replace_logger_filehandler(new_log_name):
    """
    Replace existing FileHandler in logger and preserve the original formatter for each logger.
    Also delete the original log files.

    :param new_log_name: New log file name (without suffix)
    """

    def update_handler(logger, file_suffix):
        old_log_paths = []
        formatter = None
        for handler in logger.handlers:
            if isinstance(handler, logging.FileHandler):
                if formatter is None:
                    formatter = handler.formatter
                old_log_paths.append(handler.baseFilename)

        for handler in list(logger.handlers):
            if isinstance(handler, logging.FileHandler):
                handler.close()
                logger.removeHandler(handler)

        for log_path in old_log_paths:
            if os.path.exists(log_path):
                try:
                    os.remove(log_path)
                except Exception as e:
                    pass

        if logger.name == 'MCPPredictRunLogger':
            new_name = new_log_name.replace("message", "run")
        else:
            new_name = new_log_name 

        new_handler = logging.FileHandler(f"{new_name}.{file_suffix}", mode='a', encoding='utf-8')
        if formatter:
            new_handler.setFormatter(formatter)
        logger.addHandler(new_handler)

    run_logger = logging.getLogger('MCPPredictRunLogger')
    update_handler(run_logger, 'log')

    message_logger = logging.getLogger('MCPPredictMessageLogger')
    update_handler(message_logger, 'jsonl')


def evaluate_final_answer_eval1(
            question: str, 
            ground_truth: str, 
            prediction: str, 
            manager: ProcessManager,
            logger: logging.Logger,
            ) -> Tuple[bool, Optional[str]]:
    prompt = EVAL_PROMPT_1.format(question=question, response=prediction, expected_response=ground_truth)
    messages = [
        {
            constants.ROLE: constants.USER,
            constants.CONTENT: prompt
        }
    ]
    logger.info(f"Starting evaluation of final answer")
    logger.info(f"question: {question}")
    logger.info(f"expected_response: {ground_truth}")
    logger.info(f"prediction: {prediction}")
    response_content, _, _ = call_lm(messages, manager, logger, temperature=0.01)
    return "true" in response_content.lower()



if __name__ == "__main__":
    print(question_scorer("123", "123"))
    